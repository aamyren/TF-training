{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aamyren/TF-training/blob/main/Strided-MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgF33dJqpglF",
        "outputId": "18e48e62-78fc-48c6-ea0d-9f539ea9a097"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.8.2\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
        "from tensorflow.keras import Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "metadata": {
        "id": "gcn5qOmXYvfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Add a channels dimension\n",
        "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
        "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
      ],
      "metadata": {
        "id": "nUJG9UScp_nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76aa6023-4663-4231-c795-2366555e0eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adds some background data to training\n",
        "\n",
        "# nx_train contains 30000 severely cropped and 30000 blank images to add to the training dataset\n",
        "# ----------> maybe increase this later\n",
        "nx_train = np.empty((60000,28,28,1))\n",
        "# all 40000 of these images need labels\n",
        "ny_train = np.empty(60000)\n",
        "\n",
        "# function to shift the image so that it is cropped\n",
        "def shifty(img):\n",
        "    seed = random.randrange(10,18)\n",
        "    new = np.roll(img, seed, axis = 0)\n",
        "    return new\n",
        "    \n",
        "def shiftx(img):\n",
        "    seed = random.randrange(10,18)\n",
        "    new = np.roll(img, seed, axis = 1)\n",
        "    #seed = random.randrange(84900000, 85000000)\n",
        "    #new = np.roll(img, -15, axis=1)\n",
        "    #new = np.roll(img, seed)\n",
        "    return new\n",
        "\n",
        "def shiftxy(img):\n",
        "    seed = random.randrange(10,18)\n",
        "    new = np.roll(img, seed, axis = (1,0))\n",
        "    seed = random.randrange(10,18)\n",
        "    new = np.roll(img, seed, axis = (0,1))\n",
        "    return new\n",
        "\n",
        "# changes the value of the first 60000 images to have a value \"digit\"\n",
        "for i in range(60000):\n",
        "    y_train[i] = 2\n",
        "\n",
        "# cropped images correspond to class label 0\n",
        "for i in range(30000):\n",
        "    ny_train[i] = 0\n",
        "\n",
        "# blank images correspond to class label 1\n",
        "for i in range(30000, 60000):\n",
        "    ny_train[i] = 1\n",
        "\n",
        "# changes the first 30000 images in nx_train to be cropped, the rest are blank\n",
        "for i in range(10000):\n",
        "    nx_train[i] = shifty(x_train[i])\n",
        "    #nx_train[i] = shiftx(x_train[i])\n",
        "\n",
        "for i in range(10000,20000):\n",
        "    nx_train[i] = shiftx(x_train[i])\n",
        "    #nx_train[i] = shiftx(x_train[i])\n",
        "\n",
        "for i in range(20000,30000):\n",
        "    nx_train[i] = shiftxy(x_train[i])\n",
        "    #nx_train[i] = shiftx(x_train[i])\n",
        "\n",
        "# adjust how much data from the \"digit\" class we want to keep\n",
        "mx_train = []\n",
        "my_train = []\n",
        "for i in range(30000):\n",
        "    mx_train.append(x_train[i])\n",
        "    my_train.append(y_train[i])\n",
        "\n",
        "#print(y_train)\n",
        "#print(ny_train)\n",
        "#y_train.shape\n",
        "#ny_train.shape"
      ],
      "metadata": {
        "id": "mM05IRDLYgwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adds some background data to test data\n",
        "\n",
        "# the 10000 test images should all be considered \"digits\"\n",
        "for i in range(10000):\n",
        "    y_test[i] = 1\n",
        "\n",
        "x_test.shape\n",
        "y_test.shape\n",
        "\n",
        "# add 9000 non-digits to test\n",
        "nx_test = np.zeros((9000,28,28,1))\n",
        "# all 9000 of these images correspond to \"background\"\n",
        "ny_test = np.zeros(9000)\n",
        "\n",
        "\n",
        "# changes the value of the first 10000 images to have a value \"digit\"\n",
        "for i in range(10000):\n",
        "    y_test[i] = 2\n",
        "\n",
        "# cropped images\n",
        "for i in range(4500):\n",
        "    ny_test[i] = 0\n",
        "# blank\n",
        "for i in range(4500, 9000):\n",
        "    ny_test[i] = 1\n",
        "\n",
        "\n",
        "# changes the first 4500 images in nx_test to be cropped, the rest are blank\n",
        "for i in range(1500):\n",
        "    #nx_test[i] = shift(x_test[i])\n",
        "    nx_test[i] = shifty(x_test[i])\n",
        "\n",
        "for i in range(1500, 3000):\n",
        "    #nx_test[i] = shift(x_test[i])\n",
        "    nx_test[i] = shiftx(x_test[i])\n",
        "\n",
        "for i in range(1500, 4500):\n",
        "    #nx_test[i] = shift(x_test[i])\n",
        "    nx_test[i] = shiftxy(x_test[i])\n",
        "\n",
        "\n",
        "#print(y_test)\n",
        "#print(ny_test)\n",
        "#y_test.shape\n",
        "#ny_test.shape\n",
        "\n"
      ],
      "metadata": {
        "id": "8mFUYTSKhYct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nx_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34DQkoRY4qk-",
        "outputId": "1ba9c13a-76c0-4f68-dcea-b2b91029d54b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## add the new data into x_train and y_train\n",
        "# nx_train = np.resize(nx_train, (32000, 28, 28,1))\n",
        "\n",
        "mx_train = np.concatenate((mx_train, nx_train))\n",
        "my_train = np.concatenate((my_train, ny_train))\n",
        "\n",
        "#x_train.shape\n",
        "#x_test.shape\n",
        "#y_test.shape"
      ],
      "metadata": {
        "id": "W9u3jCckd9Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = np.concatenate((x_test, nx_test))\n",
        "y_test = np.concatenate((y_test, ny_test))\n"
      ],
      "metadata": {
        "id": "dCyExPKll66y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5EozkDcQeWN",
        "outputId": "a2d1038f-af04-4c1c-e8b5-3590658fd51c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19000, 28, 28, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking how the cropped images look\n",
        "'''\n",
        "nx_train = np.resize(nx_train, (30000, 28, 28))\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(100):\n",
        "    plt.subplot(10,10,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(nx_train[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(ny_train[i])\n",
        "plt.show()'''"
      ],
      "metadata": {
        "id": "KYFgyfXCYl6j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "71ea1b16-c841-479a-d1b3-4cc9f219ab7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnx_train = np.resize(nx_train, (30000, 28, 28))\\n\\nplt.figure(figsize=(10,10))\\nfor i in range(100):\\n    plt.subplot(10,10,i+1)\\n    plt.xticks([])\\n    plt.yticks([])\\n    plt.grid(False)\\n    plt.imshow(nx_train[i], cmap=plt.cm.binary)\\n    plt.xlabel(ny_train[i])\\nplt.show()'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''nx_train = np.resize(nx_train, (60000, 28, 28))\n",
        "plt.figure()\n",
        "plt.imshow(nx_train[4738], cmap = plt.cm.binary)'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ZmK_1otnwdRR",
        "outputId": "47f1d40a-5864-407f-bf9b-95451b222882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'nx_train = np.resize(nx_train, (60000, 28, 28))\\nplt.figure()\\nplt.imshow(nx_train[4738], cmap = plt.cm.binary)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#nx_train = np.resize(nx_train, (120000, 28, 28, 1))"
      ],
      "metadata": {
        "id": "_Hx7bqkw-8Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up network for the filter, which detects background/digit\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (mx_train, my_train)).shuffle(10000).batch(32)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
      ],
      "metadata": {
        "id": "o7UDve9zqyFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Filter(Model):\n",
        "  def __init__(self):\n",
        "    super(Filter, self).__init__()\n",
        "    self.conv1 = Conv2D(32, 3, activation='relu')\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(128, activation='relu')\n",
        "    self.d2 = Dense(3)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    return self.d2(x)\n",
        "\n",
        "# Create an instance of the model\n",
        "filter = Filter()"
      ],
      "metadata": {
        "id": "gXqHdsO9rMxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "metadata": {
        "id": "p8vB27XrsKeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "metadata": {
        "id": "-Vdz1lcusVfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # training=True is only needed if there are layers with different\n",
        "    # behavior during training versus inference (e.g. Dropout).\n",
        "    predictions = filter(images, training=True)\n",
        "    loss = loss_object(labels, predictions)\n",
        "  gradients = tape.gradient(loss, filter.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, filter.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)"
      ],
      "metadata": {
        "id": "44dVmgHvsq0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  # training=False is only needed if there are layers with different\n",
        "  # behavior during training versus inference (e.g. Dropout).\n",
        "  predictions = filter(images, training=False)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "metadata": {
        "id": "95bv9CBntPqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()\n",
        "\n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "\n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  print(\n",
        "    f'Epoch {epoch + 1}, '\n",
        "    f'Loss: {train_loss.result()}, '\n",
        "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
        "    f'Test Loss: {test_loss.result()}, '\n",
        "    f'Test Accuracy: {test_accuracy.result() * 100}'\n",
        "  )"
      ],
      "metadata": {
        "id": "BSYuyfiktk43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2abe6765-a030-4d17-99f6-b0b1a740d236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.006081289146095514, Accuracy: 99.92444610595703, Test Loss: 0.0008889682358130813, Test Accuracy: 99.98420715332031\n",
            "Epoch 2, Loss: 0.0002874603378586471, Accuracy: 99.99444580078125, Test Loss: 0.001814084011130035, Test Accuracy: 99.92631530761719\n",
            "Epoch 3, Loss: 0.00031803984893485904, Accuracy: 99.99221801757812, Test Loss: 0.0021021722350269556, Test Accuracy: 99.98947143554688\n",
            "Epoch 4, Loss: 3.3331489248666912e-06, Accuracy: 100.0, Test Loss: 0.0012924371985718608, Test Accuracy: 99.99473571777344\n",
            "Epoch 5, Loss: 5.399376732384553e-07, Accuracy: 100.0, Test Loss: 0.0014458366204053164, Test Accuracy: 99.99473571777344\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking predictions\n",
        "\n",
        "#probability_model = tf.keras.Sequential([filter, \n",
        "#                                         tf.keras.layers.Softmax()])"
      ],
      "metadata": {
        "id": "hmodRpHYt3ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x_test = np.resize(x_test, (15000, 28, 28, 1))\n",
        "#filter_predictions = probability_model.predict(x_test)"
      ],
      "metadata": {
        "id": "vLvBa1a-pUyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#np.argmax(filter_predictions[0])"
      ],
      "metadata": {
        "id": "TB7DM8f3pW2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x_test = np.resize(x_test, (15000, 28, 28))\n",
        "\n",
        "#plt.figure()\n",
        "#plt.imshow(x_test[43], cmap = plt.cm.binary)\n",
        "\n",
        "#np.argmax(filter_predictions[43])\n",
        "\n",
        "# the network to identify ----digit or background---- has been completed and trained at this point "
      ],
      "metadata": {
        "id": "ljdk8Tclpd-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filter.save_weights('checkpoint_1')"
      ],
      "metadata": {
        "id": "HcQhfLBezJUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----Classifier---- neural network begins here"
      ],
      "metadata": {
        "id": "XlmSYHzP5eyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import mnist data\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(cx_train, cy_train), (cx_test, cy_test) = mnist.load_data()\n",
        "cx_train, cx_test = cx_train / 255.0, cx_test / 255.0\n",
        "\n",
        "# Add a channels dimension\n",
        "cx_train = cx_train[..., tf.newaxis].astype(\"float32\")\n",
        "cx_test = cx_test[..., tf.newaxis].astype(\"float32\")"
      ],
      "metadata": {
        "id": "4aP-6sI_51Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up classifier network to identify the digit, same as problem 1)pt2)\n",
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (cx_train, cy_train)).shuffle(10000).batch(32)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((cx_test, cy_test)).batch(32)"
      ],
      "metadata": {
        "id": "74YXJQBt519l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load weights from the filter network into the classifier network\n",
        "# filter.load_weights('checkpoint_1')"
      ],
      "metadata": {
        "id": "8gB0zMVO6Mi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Classifier(Model):\n",
        "  def __init__(self):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.conv1 = Conv2D(32, 3, activation='relu')\n",
        "    self.flatten = Flatten()\n",
        "    self.d1 = Dense(128, activation='relu')\n",
        "    self.d2 = Dense(10)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.flatten(x)\n",
        "    x = self.d1(x)\n",
        "    return self.d2(x)\n",
        "\n",
        "# Create an instance of the model\n",
        "classifier = Classifier()"
      ],
      "metadata": {
        "id": "8HCKmr993IWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "metadata": {
        "id": "CKFZbdwd4Y1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
      ],
      "metadata": {
        "id": "pwQ-Lplt4anE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "    # training=True is only needed if there are layers with different\n",
        "    # behavior during training versus inference (e.g. Dropout).\n",
        "    predictions = classifier(images, training=True)\n",
        "    loss = loss_object(labels, predictions)\n",
        "  gradients = tape.gradient(loss, classifier.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, classifier.trainable_variables))\n",
        "\n",
        "  train_loss(loss)\n",
        "  train_accuracy(labels, predictions)"
      ],
      "metadata": {
        "id": "4OBxolgU4cez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  # training=False is only needed if there are layers with different\n",
        "  # behavior during training versus inference (e.g. Dropout).\n",
        "  predictions = classifier(images, training=False)\n",
        "  t_loss = loss_object(labels, predictions)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, predictions)"
      ],
      "metadata": {
        "id": "qiiMS4sI4ckV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  # Reset the metrics at the start of the next epoch\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  test_loss.reset_states()\n",
        "  test_accuracy.reset_states()\n",
        "\n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "\n",
        "  for test_images, test_labels in test_ds:\n",
        "    test_step(test_images, test_labels)\n",
        "\n",
        "  print(\n",
        "    f'Epoch {epoch + 1}, '\n",
        "    f'Loss: {train_loss.result()}, '\n",
        "    f'Accuracy: {train_accuracy.result() * 100}, '\n",
        "    f'Test Loss: {test_loss.result()}, '\n",
        "    f'Test Accuracy: {test_accuracy.result() * 100}'\n",
        "  )"
      ],
      "metadata": {
        "id": "KXJFnvLL4gb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d070507-4cb8-4306-d3fa-78b4f40d2945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.13785535097122192, Accuracy: 95.82666778564453, Test Loss: 0.0667387843132019, Test Accuracy: 97.91999816894531\n",
            "Epoch 2, Loss: 0.04667611047625542, Accuracy: 98.51333618164062, Test Loss: 0.0518634095788002, Test Accuracy: 98.2699966430664\n",
            "Epoch 3, Loss: 0.024410484358668327, Accuracy: 99.23833465576172, Test Loss: 0.06407970935106277, Test Accuracy: 98.07999420166016\n",
            "Epoch 4, Loss: 0.015367086045444012, Accuracy: 99.50333404541016, Test Loss: 0.055316176265478134, Test Accuracy: 98.43000030517578\n",
            "Epoch 5, Loss: 0.010461768135428429, Accuracy: 99.63166809082031, Test Loss: 0.05413881689310074, Test Accuracy: 98.61000061035156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.save_weights('checkpoint_2')"
      ],
      "metadata": {
        "id": "m0D-WCYlBNYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----Classifier network---- training completed here"
      ],
      "metadata": {
        "id": "g1AhcTi3cxrv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# begin setting up evaluation data for striding \n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import scipy.io as spio\n",
        "from tqdm import trange\n",
        "from matplotlib import cm\n",
        "from keras.utils import np_utils\n",
        "from matplotlib import pyplot\n",
        "from google.colab.patches import cv2_imshow\n"
      ],
      "metadata": {
        "id": "uBPI3LfadlWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Functions to convert .mat file to image arrays (Source:Github)\n",
        "def loadmat(filename):\n",
        "    '''\n",
        "    this function should be called instead of direct spio.loadmat\n",
        "    as it cures the problem of not properly recovering python dictionaries\n",
        "    from mat files. It calls the function check keys to cure all entries\n",
        "    which are still mat-objects\n",
        "    '''\n",
        "    data = spio.loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
        "    return _check_keys(data)\n",
        "\n",
        "\n",
        "def _check_keys(dict):\n",
        "    '''\n",
        "    checks if entries in dictionary are mat-objects. If yes\n",
        "    todict is called to change them to nested dictionaries\n",
        "    '''\n",
        "    for key in dict:\n",
        "        if isinstance(dict[key], spio.matlab.mio5_params.mat_struct):\n",
        "            dict[key] = _todict(dict[key])\n",
        "    return dict        \n",
        "\n",
        "\n",
        "def _todict(matobj):\n",
        "    #A recursive function which constructs from matobjects nested dictionaries\n",
        "    dict = {}\n",
        "    for strg in matobj._fieldnames:\n",
        "        elem = matobj.__dict__[strg]\n",
        "        if isinstance(elem, spio.matlab.mio5_params.mat_struct):\n",
        "            dict[strg] = _todict(elem)\n",
        "        else:\n",
        "            dict[strg] = elem\n",
        "    return dict\n",
        "\n",
        "def visualization(vector, vector_name):\n",
        "    y = np.reshape(vector, (40, 40))\n",
        "    plt.imshow(y, cmap=cm.Greys_r)\n",
        "    plt.suptitle(vector_name)\n",
        "    plt.axis('off')\n",
        "    plt.pause(0.0001)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MM1X2DwAdxwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "oc66tjiROhnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e486ff8-86a7-49b4-f62f-983ad124e228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define number of classes\n",
        "nb_classes = 10\n",
        "\n",
        "#Load .mat data\n",
        "dataset = loadmat(\"/content/drive/My Drive/training_and_validation_batches/1.mat\")\n",
        "\n",
        "#Extract image array and the corresponding label   \n",
        "y_val = dataset['affNISTdata']['label_int']\n",
        "X_val = dataset['affNISTdata']['image'].transpose()\n",
        "\n",
        "#Reshape and preprocess label\n",
        "X_val = X_val.reshape(60000, 40, 40)\n",
        "X_val = X_val.astype(\"float64\")\n",
        "y_val = np_utils.to_categorical(y_val, nb_classes)\n",
        "\n",
        "#Save Files in .npy format\n",
        "np.save(\"/content/drive/My Drive/Colab Notebooks/Completed/affNIST_X_val\", X_val)\n",
        "np.save(\"/content/drive/My Drive/Colab Notebooks/Completed/affNIST_y_val\", y_val)\n"
      ],
      "metadata": {
        "id": "oHfktkuydyXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how the imported data looks\n",
        "\n",
        "#plt.figure(figsize=(10,10))\n",
        "#for i in range(100):\n",
        "#    plt.subplot(10,10,i+1)\n",
        "#    plt.xticks([])\n",
        "#    plt.yticks([])\n",
        "#    plt.grid(False)\n",
        "#    plt.imshow(X_val[i], cmap=plt.cm.binary)\n",
        "#    plt.xlabel(y_val[i])\n",
        "#plt.show()"
      ],
      "metadata": {
        "id": "OhmXdMRejwwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = np.resize(X_val, (60000, 40, 40))"
      ],
      "metadata": {
        "id": "8VIAr2pMG7vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# small strider returns array of all strided images of a given index of X_val\n",
        "def s_strider(j):\n",
        "    s_list = []\n",
        "    coord = []\n",
        "    strides = np.empty((144, 28, 28)) # first index is number of total strides\n",
        "    for x in range(0,12,1): # 3rd index is step size of strides\n",
        "        for y in range(0,12,1):\n",
        "            s = X_val[j][x:x+28,y:y+28]\n",
        "            #plt.figure()\n",
        "            #plt.imshow(s, cmap = plt.cm.binary)\n",
        "            s_list.append(s)\n",
        "            coord.append([x,y])\n",
        "    for i in range(144):\n",
        "        if strides[i].shape == s_list[i].shape:\n",
        "            strides[i] = s_list[i]\n",
        "        else:\n",
        "            break\n",
        "    strides = np.resize(strides, (144, 28, 28, 1))\n",
        "    #return s_list\n",
        "    return strides"
      ],
      "metadata": {
        "id": "KTJSN7sYG74Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up predictions from the filter network\n",
        "f_probability_model = tf.keras.Sequential([filter, \n",
        "                                         tf.keras.layers.Softmax()])\n",
        "# can change the number for strider() based on index in X_val to check\n",
        "s = s_strider(1600) \n",
        "stride_filter_predictions = f_probability_model.predict(s)    \n",
        "#print(np.argmax(stride_filter_predictions))      \n",
        "\n",
        "s_predict = []\n",
        "\n",
        "# manually display output of striding\n",
        "s = np.resize(s, (144, 28, 28))\n",
        "\n",
        "import statistics\n",
        "b = 0\n",
        "results = []\n",
        "for i in range(0,144):\n",
        "    s_predict.append(np.max(stride_filter_predictions))\n",
        "    if np.argmax(stride_filter_predictions[i]) == 2:\n",
        "        results.append(i)\n",
        "        #b = i\n",
        "        #plt.figure()\n",
        "        #plt.imshow(s[b], cmap = plt.cm.binary)\n",
        "        #print(b)    \n",
        "\n",
        "#if len(results) > 20:\n",
        "#    b = results[1]\n",
        "#else:\n",
        "#    b = results[-2]\n",
        "b = int(statistics.median(results))\n",
        "plt.figure()\n",
        "plt.imshow(s[b], cmap = plt.cm.binary)\n",
        "#print(b)    \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "s98WSV2pHK8O",
        "outputId": "5a9638d6-b06a-44a5-f545-278c54e401d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6bd9b9c0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 561
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# identify 1-10 the cropped digits\n",
        "# set up predictions from the classifier network\n",
        "c_probability_model = tf.keras.Sequential([classifier, \n",
        "                                         tf.keras.layers.Softmax()]) \n",
        "s = np.resize(s, (144, 28, 28, 1))\n",
        "stride_classifier_predictions = c_probability_model.predict(s)      \n",
        "print(\"The digit above is : \", np.argmax(stride_classifier_predictions))\n",
        "#print(stride_classifier_predictions[b])"
      ],
      "metadata": {
        "id": "xL4WRos7dpPN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9a83fc5-e28b-4264-bfad-ff1a8edc2984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The digit above is :  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluations of 256x256 images begin here --------"
      ],
      "metadata": {
        "id": "Qf1Tt_kcn8vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val = np.resize(X_val, (60000, 40, 40, 1))\n",
        "\n",
        "# make 256x256 image\n",
        "# i am making them 1 at a timebecause session crashes from too much RAM useage otherwise\n",
        "large_x = np.zeros((1,256,256,1))\n",
        "\n",
        "# place an index of X_val randomly somewhere in 256x256 image\n",
        "x = random.randrange(216)\n",
        "# print (x, y)\n",
        "y = random.randrange(216)\n",
        "large_x[0][x:x+X_val.shape[1], y:y+X_val.shape[2]] = X_val[888] # can change index of X_val"
      ],
      "metadata": {
        "id": "yoynIGT3fwmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# display 256x256 sized figure\n",
        "large_x = np.resize(large_x, (1, 256, 256))\n",
        "plt.figure()\n",
        "plt.imshow(large_x[0], cmap = plt.cm.binary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "cGSqnnNzFCLl",
        "outputId": "567bf405-fe9a-4b4f-8e45-7a80871b5cc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f6bd7b6dd50>"
            ]
          },
          "metadata": {},
          "execution_count": 604
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# large strider returns array of all strided images of a given index of X_val\n",
        "def l_strider(j):\n",
        "    s_list = []\n",
        "    coord = []\n",
        "    strides = np.empty((3249, 28, 28)) # first index is number of total strides\n",
        "    for x in range(0,228,4): # 3rd index is step size of strides\n",
        "        for y in range(0,228,4):\n",
        "            s = large_x[0][x:x+28,y:y+28]\n",
        "            #plt.figure()\n",
        "            #plt.imshow(s, cmap = plt.cm.binary)\n",
        "            s_list.append(s)\n",
        "            coord.append([x,y])\n",
        "    for i in range(3249):\n",
        "        if strides[i].shape == s_list[i].shape:\n",
        "            strides[i] = s_list[i]\n",
        "        else:\n",
        "            break\n",
        "    strides = np.resize(strides, (3249, 28, 28, 1))\n",
        "    #return s_list\n",
        "\n",
        "    return strides"
      ],
      "metadata": {
        "id": "WNBIN2Go5E3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statistics"
      ],
      "metadata": {
        "id": "PbEclDlto9Mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up predictions from the filter network\n",
        "f_probability_model = tf.keras.Sequential([filter, \n",
        "                                         tf.keras.layers.Softmax()])\n",
        "# can change the number for strider() based on index in X_val to check\n",
        "l_s = l_strider(12)\n",
        "l_stride_filter_predictions = f_probability_model.predict(l_s)          \n",
        "\n",
        "# manually display output of striding\n",
        "l_s = np.resize(l_s, (3249, 28, 28))\n",
        "\n",
        "# find which strided boxes have predictions correspond to class 2 (digit only)\n",
        "b2 = 0\n",
        "results2 = []\n",
        "for i in range(0,3249):\n",
        "    if np.argmax(l_stride_filter_predictions[i]) == 2:\n",
        "        results2.append(i)\n",
        "        #b = i\n",
        "        #plt.figure()\n",
        "        #plt.imshow(l_s[b2], cmap = plt.cm.binary)\n",
        "        #print(b2)    \n",
        "\n",
        "# i take the median of predictions in case there are multiple boxes corresponding to digit- generally the image will be centered this way\n",
        "b2 = int(statistics.median(results2))\n",
        "# manual check \n",
        "l_s = np.resize(l_s, (3249, 28, 28))\n",
        "plt.figure()\n",
        "plt.imshow(l_s[b2], cmap = plt.cm.binary)\n",
        "print(b2)"
      ],
      "metadata": {
        "id": "VBea4Sfdm7PB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "01fb2c09-fe34-43c8-9a6f-f7c2c5234af9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "130\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# identify 1-10 the cropped digits\n",
        "\n",
        "# set up predictions from the classifier network\n",
        "#c2_probability_model = tf.keras.Sequential([classifier, \n",
        "#                                         tf.keras.layers.Softmax()]) \n",
        "l_s = np.resize(l_s, (3249, 28, 28, 1))\n",
        "stride2_classifier_predictions = c_probability_model.predict(l_s)      \n",
        "print(\"The digit above is : \", np.argmax(stride2_classifier_predictions[b2]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7imu7KR5jX5T",
        "outputId": "1767201a-44bf-4a72-e5e9-623e52649e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The digit above is :  6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list with same structure as the strider loops to help find coordinates based on the output index\n",
        "a = []\n",
        "for x in range(0,228,4):\n",
        "    for y in range(0,228,4):\n",
        "        s = X_val[2][x:x+28,y:y+28]\n",
        "        a.append((x,y))\n",
        "\n",
        "# len(a)\n",
        "# a[b2]\n"
      ],
      "metadata": {
        "id": "aKbX4UmUScW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# draw box around figure\n",
        "from google.colab.patches import cv2_imshow\n",
        "# initiates blank array every time so that rectangles aren't drawn directly on the data and so that drawings can be reset each time\n",
        "drawing = np.empty((1, 256, 256))\n",
        "# draw bounding rectangle based on coordinates found in list a\n",
        "drawing[0] = large_x[0] \n",
        "image = drawing[0]\n",
        "start_point = (a[b2][1],a[b2][0])\n",
        "end_point = (a[b2][1]+28, a[b2][0]+28)\n",
        "color = (255, 0, 0)\n",
        "thickness = 2\n",
        "image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
        "cv2_imshow(image) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "pCS26U_GUtMh",
        "outputId": "b281c7b8-6245-4a0c-bd4f-6110505ee673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=256x256 at 0x7F6BD9511FD0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQAAAAEACAAAAAB5Gfe6AAACRElEQVR4nO3ZX2jNYRwH4HdnO1sbmz8pNRc2+0Nm5A5zJyU32g0RSrmXSyUpV27dIEW5kFjulmiSRkiKJdSa2hIrhyVbwnRc2HTM2dB5zzo753luP51Pvd/z/t731zkhAEDJKsu1IJ3X9vxL5Pj5Gdf/l7Qg5DqAOa8i94pp9/kc+P7zswMWlOehNF8i7IBMlYvGdy75sqv7WNzaPMr1nE5nVJQl2zvrOkb7Ux9670/NClbEAdQ2bO18/+zmYPrtt/LvU7LCFW0AVW3bWpZeefDqa5askMUawPLd+3v6u96NZ8uKWjqdDiEk2i4M7KvPnhW5n4tceaZ3759335wYQJz3gKbExe7vUZpmXZwBrBi9OxKlaPbFGUDfpvryquqWNQtDsiEZpXHWxHkTfDxwIHWw400q0fFk5HJPlMrZEucaLN9+dHjZi9vJTxV1q9aNXbsxmJEVuYmTvvbk0PnWyhDKauY17zt9vD2RkRW3yUW2nR063TRxoLR1XWrNzIra5CITLadGbx9uTYQQQmLb1UMVocQGEEL12iMvz28IIYRQ2Xnv96yAxfs94HPf6497Lj+9fmdobH5javXzaMX5FfP3gFC3cfOOmoe94+tbes8Nl9Yt8Etyy/Fb3x6daMyWFaWsi2xePH1WaKI+Av+RFYyS/1+g5AcQ4RqcAw/6DEp+B+Q6gJlPucI/AwEAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPhnPwDtnp7qfSU9kgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}