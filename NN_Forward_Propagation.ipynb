{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM892m49B7OQoMQziPF6w8I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aamyren/TF-training/blob/main/NN_Forward_Propagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzZHMyKMzP75",
        "outputId": "b11ab068-46fb-4561-aaed-9b96eb4fc0b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30.0]\n",
            "[1.0, 70.3]\n",
            "[8.005945856232704e-31, 1.0, 109.5]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.7847500457914554e-48, 7.569730354473997e-48, 1.0]"
            ]
          },
          "metadata": {},
          "execution_count": 313
        }
      ],
      "source": [
        "class Tensor():\n",
        "    def __init__(self, data, shape):\n",
        "        self.data = data\n",
        "        self.shape = shape\n",
        "\n",
        "    def shape_data(self):\n",
        "        # outputs an empty list if shape is an empty list\n",
        "        if self.shape == []:\n",
        "            return []\n",
        "        # make sure data is int or float, and shape is a list of positive integers\n",
        "        for s in self.shape:\n",
        "            if type(s) != int or s < 1:\n",
        "                print('Shape needs to be a list of positive integers')\n",
        "                return\n",
        "        for d in self.data:\n",
        "            if type(d) != int and type(d) != float:\n",
        "                print('Data needs to be a number (int or float)')\n",
        "                return\n",
        "        # calculates dimension of the tensor\n",
        "        dim = 1\n",
        "        for j in self.shape:\n",
        "            dim = dim * j\n",
        "        # stores data in data_list and modified according to specified dimensions\n",
        "        data_list = self.data\n",
        "        #if not enough data numbers, pad tensor w/ 0s\n",
        "        if len(self.data) < dim:\n",
        "            for i in range(dim - len(self.data)):\n",
        "                data_list.append(0)\n",
        "        #if too many data numbers, cut off after tensor fills up\n",
        "        elif len(self.data) >= dim:\n",
        "            del data_list[dim:]\n",
        "\n",
        "        if self.shape[0] == 1:\n",
        "            return self.data\n",
        "\n",
        "        # slices the data list into groups specified by shape, starting with the last index\n",
        "        # the loop repeats for each data point (excluding the first b/c dimension has already been accounted for)\n",
        "        # updates the sliced list every iteration\n",
        "        for j in range(1, len(self.shape)):\n",
        "            data_list = ([data_list[x:x+self.shape[-1*j]] for x in range(0, len(data_list), self.shape[-1*j])])\n",
        "\n",
        "        #prints the constructed tensor\n",
        "        return(data_list)       \n",
        "\n",
        "# initializing new Dense class\n",
        "class Dense():\n",
        "    def __init__(self, inputs, weights, biases):\n",
        "        self.inputs = inputs\n",
        "        self.weights = weights\n",
        "        self.biases = biases\n",
        "\n",
        "    # function that performs the calculations\n",
        "    def forward(self, input, weights, bias):\n",
        "        #output = 0\n",
        "        output = (float(input) * float(weights)) + float(bias)\n",
        "        #print(output)\n",
        "        return(output)\n",
        "\n",
        "    def softmax(self, vector):\n",
        "        e = 2.71828\n",
        "        exp = []\n",
        "        output = []\n",
        "        #exp = e**vector\n",
        "        for i in range(len(vector)):\n",
        "            exp.append(e**vector[i])\n",
        "        sum = 0\n",
        "        for i in range(len(exp)):\n",
        "            sum += exp[i]\n",
        "        for i in range(len(exp)):\n",
        "            output.append(exp[i]/sum)\n",
        "        return output\n",
        "\n",
        "    def layer(self, inputs, weights, biases):\n",
        "        layer = []\n",
        "        for n_weights, n_bias in list(zip(weights, biases)):\n",
        "            output = 0\n",
        "            #print (n_weights, n_bias)\n",
        "            for n_input, i_weight in list(zip(inputs, n_weights)):\n",
        "                output += self.forward(n_input, i_weight, n_bias/(len(inputs)))\n",
        "                #print (output)\n",
        "                #print (n_input, i_weight, n_bias)\n",
        "                #print(output)\n",
        "            layer.append(output)\n",
        "            print(layer)\n",
        "            layer = self.softmax(layer)\n",
        "        #print(layer)\n",
        "        return layer\n",
        "\n",
        "# Some manual checks for the Dense() class\n",
        "x0 = Tensor([1,1],[1,4])\n",
        "w0 = Tensor([1,1,1,1],[2,2])\n",
        "b0 = Tensor([0,0.5],[1,2])\n",
        "x = Tensor.shape_data(x0)\n",
        "#print(x)\n",
        "w = Tensor.shape_data(w0)\n",
        "#print(w)\n",
        "b = Tensor.shape_data(b0)\n",
        "#print(b)\n",
        "\n",
        "\n",
        "x = [1,2,3,4]\n",
        "w = [[1,2,3,4],[5,6,7,8],[9,10,11,12]]\n",
        "b = [0,0.3,-0.5]\n",
        "\n",
        "dense0 = Dense(x, w, b)\n",
        "dense0.layer(x,w,b)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for testing the softmax output math for function w/o importing libraries\n",
        "# this one imports numpy \n",
        "from numpy import exp\n",
        " \n",
        "def softmax(vector):\n",
        "  e = exp(vector)\n",
        "  # print(e)\n",
        "  output = e / e.sum()\n",
        "  #print(output)\n",
        "  return(output)\n",
        "\n",
        "a = [1, 3, 2]\n",
        "softmax(a)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJ4lHYbbpdNl",
        "outputId": "4d2dc721-9502-4ae5-f388-e4455e282e6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.09003057, 0.66524096, 0.24472847])"
            ]
          },
          "metadata": {},
          "execution_count": 458
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(softmax(a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I7WSmALdX1n",
        "outputId": "dab1fa96-cd4f-44ae-fbbd-4ac8447c7e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 2.71828183 20.08553692  7.3890561 ]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}